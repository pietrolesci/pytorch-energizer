{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.special as sp\n",
    "from datasets import load_dataset\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "from energizer.data import ActiveDataModule, ActiveDataset\n",
    "from energizer.inference import Deterministic\n",
    "from energizer.loops import ActiveLearningLoop\n",
    "from energizer.strategies import LeastConfidenceStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "ds = load_dataset(\"pietrolesci/ag_news\", name=\"concat\")\n",
    "ds = ds.map(lambda ex: tokenizer(ex[\"text\"], return_token_type_ids=False), batched=True)\n",
    "ds = ds.with_format(columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=4)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.backbone(**batch).logits\n",
    "\n",
    "    def step(self, batch, *args, **kwargs):\n",
    "        y = batch.pop(\"labels\")\n",
    "        y_hat = self(batch)\n",
    "        return self.loss(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        # self.print(\"TRAIN\")\n",
    "        loss = self.step(batch, *args, **kwargs)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        # self.print(\"VAL\")\n",
    "        loss = self.step(batch, *args, **kwargs)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, *args, **kwargs):\n",
    "        # self.print(\"TEST\")\n",
    "        loss = self.step(batch, *args, **kwargs)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, *args, **kwargs):\n",
    "        # self.print(\"PREDICT\")\n",
    "        return self(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def on_pool_batch_end(self, *args, **kwargs):\n",
    "        self.print(\"PRINTING FROM MODULE\")\n",
    "\n",
    "\n",
    "class ActiveLearninigCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_pool_batch_end(self, *args, **kwargs):\n",
    "        print(\"FROM THE CALLBACK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dm():\n",
    "    train_ds = ds[\"train\"].select(list(range(1_000)))\n",
    "    return ActiveDataModule(\n",
    "        num_classes=4,\n",
    "        train_dataset=train_ds,\n",
    "        # val_dataset=ds[\"test\"],\n",
    "        initial_labels=np.random.choice(list(range(len(train_ds))), size=100, replace=False).tolist(),\n",
    "        val_split=0.5,\n",
    "        test_dataset=ds[\"test\"].select(range(1_000)),\n",
    "        # predict_dataset=ds[\"test\"].select(range(15)),\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1111)\n",
    "\n",
    "# get data\n",
    "dm = get_dm()\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    callbacks=ActiveLearninigCallback(),\n",
    ")\n",
    "\n",
    "# define active learning loop and attach to trainer\n",
    "active_learning_loop = ActiveLearningLoop(\n",
    "    al_strategy=LeastConfidenceStrategy(inference_module=Deterministic()),\n",
    "    query_size=2,\n",
    "    reset_weights=True,\n",
    "    n_epochs_between_labelling=3,\n",
    ")\n",
    "active_learning_loop.connect(trainer)\n",
    "trainer.fit_loop = active_learning_loop\n",
    "\n",
    "# fit model with active learning\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "195fd7177374df90e8fb0ddf6905c3c94c4a4300f1cc015456754f40bbdfd90b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('energizer-dev': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
